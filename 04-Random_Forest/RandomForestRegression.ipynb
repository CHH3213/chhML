{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 构建随机森林回归模型\n"]},{"cell_type":"markdown","metadata":{},"source":["导入工具库"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn import preprocessing\n","from sklearn.ensemble import RandomForestRegressor\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["加载数据"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["\n","from sklearn.datasets import load_boston"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["boston_house = load_boston()"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["boston_feature_name = boston_house.feature_names\n","boston_features = boston_house.data\n","boston_target = boston_house.target"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n","       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["boston_feature_name"]},{"cell_type":"markdown","metadata":{},"source":["查看数据描述"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[".. _boston_dataset:\n","\n","Boston house prices dataset\n","---------------------------\n","\n","**Data Set Characteristics:**  \n","\n","    :Number of Instances: 506 \n","\n","    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n","\n","    :Attribute Information (in order):\n","        - CRIM     per capita crime rate by town\n","        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n","        - INDUS    proportion of non-retail business acres per town\n","        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n","        - NOX      nitric oxides concentration (parts per 10 million)\n","        - RM       average number of rooms per dwelling\n","        - AGE      proportion of owner-occupied units built prior to 1940\n","        - DIS      weighted distances to five Boston employment centres\n","        - RAD      index of accessibility to radial highways\n","        - TAX      full-value property-tax rate per $10,000\n","        - PTRATIO  pupil-teacher ratio by town\n","        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n","        - LSTAT    % lower status of the population\n","        - MEDV     Median value of owner-occupied homes in $1000's\n","\n","    :Missing Attribute Values: None\n","\n","    :Creator: Harrison, D. and Rubinfeld, D.L.\n","\n","This is a copy of UCI ML housing dataset.\n","https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n","\n","\n","This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n","\n","The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n","prices and the demand for clean air', J. Environ. Economics & Management,\n","vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n","...', Wiley, 1980.   N.B. Various transformations are used in the table on\n","pages 244-261 of the latter.\n","\n","The Boston house-price data has been used in many machine learning papers that address regression\n","problems.   \n","     \n",".. topic:: References\n","\n","   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n","   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n","\n"]}],"source":["print(boston_house.DESCR)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n","        4.9800e+00],\n","       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n","        9.1400e+00],\n","       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n","        4.0300e+00],\n","       ...,\n","       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n","        5.6400e+00],\n","       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n","        6.4800e+00],\n","       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n","        7.8800e+00]])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["boston_features"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"data":{"text/plain":["array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n","       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n","       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n","       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n","       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n","       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n","       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n","       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n","       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n","       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n","       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n","       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n","       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n","       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n","       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n","       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n","       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n","       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n","       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n","       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n","       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n","       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n","       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n","       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n","       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n","       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n","       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n","       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n","       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n","       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n","       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n","       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n","       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n","       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n","       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n","        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n","       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n","       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n","        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n","        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n","       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n","       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n","       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n","       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n","       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n","       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["boston_target"]},{"cell_type":"markdown","metadata":{},"source":["### 构建模型"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on class RandomForestRegressor in module sklearn.ensemble._forest:\n","\n","class RandomForestRegressor(ForestRegressor)\n"," |  A random forest regressor.\n"," |  \n"," |  A random forest is a meta estimator that fits a number of classifying\n"," |  decision trees on various sub-samples of the dataset and uses averaging\n"," |  to improve the predictive accuracy and control over-fitting.\n"," |  The sub-sample size is controlled with the `max_samples` parameter if\n"," |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n"," |  each tree.\n"," |  \n"," |  Read more in the :ref:`User Guide <forest>`.\n"," |  \n"," |  Parameters\n"," |  ----------\n"," |  n_estimators : int, default=100\n"," |      The number of trees in the forest.\n"," |  \n"," |      .. versionchanged:: 0.22\n"," |         The default value of ``n_estimators`` changed from 10 to 100\n"," |         in 0.22.\n"," |  \n"," |  criterion : {\"mse\", \"mae\"}, default=\"mse\"\n"," |      The function to measure the quality of a split. Supported criteria\n"," |      are \"mse\" for the mean squared error, which is equal to variance\n"," |      reduction as feature selection criterion, and \"mae\" for the mean\n"," |      absolute error.\n"," |  \n"," |      .. versionadded:: 0.18\n"," |         Mean Absolute Error (MAE) criterion.\n"," |  \n"," |  max_depth : int, default=None\n"," |      The maximum depth of the tree. If None, then nodes are expanded until\n"," |      all leaves are pure or until all leaves contain less than\n"," |      min_samples_split samples.\n"," |  \n"," |  min_samples_split : int or float, default=2\n"," |      The minimum number of samples required to split an internal node:\n"," |  \n"," |      - If int, then consider `min_samples_split` as the minimum number.\n"," |      - If float, then `min_samples_split` is a fraction and\n"," |        `ceil(min_samples_split * n_samples)` are the minimum\n"," |        number of samples for each split.\n"," |  \n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |  \n"," |  min_samples_leaf : int or float, default=1\n"," |      The minimum number of samples required to be at a leaf node.\n"," |      A split point at any depth will only be considered if it leaves at\n"," |      least ``min_samples_leaf`` training samples in each of the left and\n"," |      right branches.  This may have the effect of smoothing the model,\n"," |      especially in regression.\n"," |  \n"," |      - If int, then consider `min_samples_leaf` as the minimum number.\n"," |      - If float, then `min_samples_leaf` is a fraction and\n"," |        `ceil(min_samples_leaf * n_samples)` are the minimum\n"," |        number of samples for each node.\n"," |  \n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |  \n"," |  min_weight_fraction_leaf : float, default=0.0\n"," |      The minimum weighted fraction of the sum total of weights (of all\n"," |      the input samples) required to be at a leaf node. Samples have\n"," |      equal weight when sample_weight is not provided.\n"," |  \n"," |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n"," |      The number of features to consider when looking for the best split:\n"," |  \n"," |      - If int, then consider `max_features` features at each split.\n"," |      - If float, then `max_features` is a fraction and\n"," |        `round(max_features * n_features)` features are considered at each\n"," |        split.\n"," |      - If \"auto\", then `max_features=n_features`.\n"," |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n"," |      - If \"log2\", then `max_features=log2(n_features)`.\n"," |      - If None, then `max_features=n_features`.\n"," |  \n"," |      Note: the search for a split does not stop until at least one\n"," |      valid partition of the node samples is found, even if it requires to\n"," |      effectively inspect more than ``max_features`` features.\n"," |  \n"," |  max_leaf_nodes : int, default=None\n"," |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n"," |      Best nodes are defined as relative reduction in impurity.\n"," |      If None then unlimited number of leaf nodes.\n"," |  \n"," |  min_impurity_decrease : float, default=0.0\n"," |      A node will be split if this split induces a decrease of the impurity\n"," |      greater than or equal to this value.\n"," |  \n"," |      The weighted impurity decrease equation is the following::\n"," |  \n"," |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n"," |                              - N_t_L / N_t * left_impurity)\n"," |  \n"," |      where ``N`` is the total number of samples, ``N_t`` is the number of\n"," |      samples at the current node, ``N_t_L`` is the number of samples in the\n"," |      left child, and ``N_t_R`` is the number of samples in the right child.\n"," |  \n"," |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n"," |      if ``sample_weight`` is passed.\n"," |  \n"," |      .. versionadded:: 0.19\n"," |  \n"," |  min_impurity_split : float, default=None\n"," |      Threshold for early stopping in tree growth. A node will split\n"," |      if its impurity is above the threshold, otherwise it is a leaf.\n"," |  \n"," |      .. deprecated:: 0.19\n"," |         ``min_impurity_split`` has been deprecated in favor of\n"," |         ``min_impurity_decrease`` in 0.19. The default value of\n"," |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n"," |         will be removed in 1.0 (renaming of 0.25).\n"," |         Use ``min_impurity_decrease`` instead.\n"," |  \n"," |  bootstrap : bool, default=True\n"," |      Whether bootstrap samples are used when building trees. If False, the\n"," |      whole dataset is used to build each tree.\n"," |  \n"," |  oob_score : bool, default=False\n"," |      Whether to use out-of-bag samples to estimate the generalization score.\n"," |      Only available if bootstrap=True.\n"," |  \n"," |  n_jobs : int, default=None\n"," |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n"," |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n"," |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n"," |      context. ``-1`` means using all processors. See :term:`Glossary\n"," |      <n_jobs>` for more details.\n"," |  \n"," |  random_state : int, RandomState instance or None, default=None\n"," |      Controls both the randomness of the bootstrapping of the samples used\n"," |      when building trees (if ``bootstrap=True``) and the sampling of the\n"," |      features to consider when looking for the best split at each node\n"," |      (if ``max_features < n_features``).\n"," |      See :term:`Glossary <random_state>` for details.\n"," |  \n"," |  verbose : int, default=0\n"," |      Controls the verbosity when fitting and predicting.\n"," |  \n"," |  warm_start : bool, default=False\n"," |      When set to ``True``, reuse the solution of the previous call to fit\n"," |      and add more estimators to the ensemble, otherwise, just fit a whole\n"," |      new forest. See :term:`the Glossary <warm_start>`.\n"," |  \n"," |  ccp_alpha : non-negative float, default=0.0\n"," |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n"," |      subtree with the largest cost complexity that is smaller than\n"," |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n"," |      :ref:`minimal_cost_complexity_pruning` for details.\n"," |  \n"," |      .. versionadded:: 0.22\n"," |  \n"," |  max_samples : int or float, default=None\n"," |      If bootstrap is True, the number of samples to draw from X\n"," |      to train each base estimator.\n"," |  \n"," |      - If None (default), then draw `X.shape[0]` samples.\n"," |      - If int, then draw `max_samples` samples.\n"," |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n"," |        `max_samples` should be in the interval `(0, 1)`.\n"," |  \n"," |      .. versionadded:: 0.22\n"," |  \n"," |  Attributes\n"," |  ----------\n"," |  base_estimator_ : DecisionTreeRegressor\n"," |      The child estimator template used to create the collection of fitted\n"," |      sub-estimators.\n"," |  \n"," |  estimators_ : list of DecisionTreeRegressor\n"," |      The collection of fitted sub-estimators.\n"," |  \n"," |  feature_importances_ : ndarray of shape (n_features,)\n"," |      The impurity-based feature importances.\n"," |      The higher, the more important the feature.\n"," |      The importance of a feature is computed as the (normalized)\n"," |      total reduction of the criterion brought by that feature.  It is also\n"," |      known as the Gini importance.\n"," |  \n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |  \n"," |  n_features_ : int\n"," |      The number of features when ``fit`` is performed.\n"," |  \n"," |  n_outputs_ : int\n"," |      The number of outputs when ``fit`` is performed.\n"," |  \n"," |  oob_score_ : float\n"," |      Score of the training dataset obtained using an out-of-bag estimate.\n"," |      This attribute exists only when ``oob_score`` is True.\n"," |  \n"," |  oob_prediction_ : ndarray of shape (n_samples,)\n"," |      Prediction computed with out-of-bag estimate on the training set.\n"," |      This attribute exists only when ``oob_score`` is True.\n"," |  \n"," |  See Also\n"," |  --------\n"," |  DecisionTreeRegressor, ExtraTreesRegressor\n"," |  \n"," |  Notes\n"," |  -----\n"," |  The default values for the parameters controlling the size of the trees\n"," |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n"," |  unpruned trees which can potentially be very large on some data sets. To\n"," |  reduce memory consumption, the complexity and size of the trees should be\n"," |  controlled by setting those parameter values.\n"," |  \n"," |  The features are always randomly permuted at each split. Therefore,\n"," |  the best found split may vary, even with the same training data,\n"," |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n"," |  of the criterion is identical for several splits enumerated during the\n"," |  search of the best split. To obtain a deterministic behaviour during\n"," |  fitting, ``random_state`` has to be fixed.\n"," |  \n"," |  The default value ``max_features=\"auto\"`` uses ``n_features``\n"," |  rather than ``n_features / 3``. The latter was originally suggested in\n"," |  [1], whereas the former was more recently justified empirically in [2].\n"," |  \n"," |  References\n"," |  ----------\n"," |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n"," |  \n"," |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n"," |         trees\", Machine Learning, 63(1), 3-42, 2006.\n"," |  \n"," |  Examples\n"," |  --------\n"," |  >>> from sklearn.ensemble import RandomForestRegressor\n"," |  >>> from sklearn.datasets import make_regression\n"," |  >>> X, y = make_regression(n_features=4, n_informative=2,\n"," |  ...                        random_state=0, shuffle=False)\n"," |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n"," |  >>> regr.fit(X, y)\n"," |  RandomForestRegressor(...)\n"," |  >>> print(regr.predict([[0, 0, 0, 0]]))\n"," |  [-8.32987858]\n"," |  \n"," |  Method resolution order:\n"," |      RandomForestRegressor\n"," |      ForestRegressor\n"," |      sklearn.base.RegressorMixin\n"," |      BaseForest\n"," |      sklearn.base.MultiOutputMixin\n"," |      sklearn.ensemble._base.BaseEnsemble\n"," |      sklearn.base.MetaEstimatorMixin\n"," |      sklearn.base.BaseEstimator\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |  \n"," |  __abstractmethods__ = frozenset()\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from ForestRegressor:\n"," |  \n"," |  predict(self, X)\n"," |      Predict regression target for X.\n"," |      \n"," |      The predicted regression target of an input sample is computed as the\n"," |      mean predicted regression targets of the trees in the forest.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The predicted values.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.RegressorMixin:\n"," |  \n"," |  score(self, X, y, sample_weight=None)\n"," |      Return the coefficient of determination :math:`R^2` of the\n"," |      prediction.\n"," |      \n"," |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n"," |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n"," |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n"," |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n"," |      can be negative (because the model can be arbitrarily worse). A\n"," |      constant model that always predicts the expected value of `y`,\n"," |      disregarding the input features, would get a :math:`R^2` score of\n"," |      0.0.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Test samples. For some estimators this may be a precomputed\n"," |          kernel matrix or a list of generic objects instead with shape\n"," |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n"," |          is the number of samples used in the fitting for the estimator.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          True values for `X`.\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      score : float\n"," |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n"," |      \n"," |      Notes\n"," |      -----\n"," |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n"," |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n"," |      with default value of :func:`~sklearn.metrics.r2_score`.\n"," |      This influences the ``score`` method of all the multioutput\n"," |      regressors (except for\n"," |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from sklearn.base.RegressorMixin:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from BaseForest:\n"," |  \n"," |  apply(self, X)\n"," |      Apply trees in the forest to X, return leaf indices.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      X_leaves : ndarray of shape (n_samples, n_estimators)\n"," |          For each datapoint x in X and for each tree in the forest,\n"," |          return the index of the leaf x ends up in.\n"," |  \n"," |  decision_path(self, X)\n"," |      Return the decision path in the forest.\n"," |      \n"," |      .. versionadded:: 0.18\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      indicator : sparse matrix of shape (n_samples, n_nodes)\n"," |          Return a node indicator matrix where non zero elements indicates\n"," |          that the samples goes through the nodes. The matrix is of CSR\n"," |          format.\n"," |      \n"," |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n"," |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n"," |          gives the indicator value for the i-th estimator.\n"," |  \n"," |  fit(self, X, y, sample_weight=None)\n"," |      Build a forest of trees from the training set (X, y).\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The training input samples. Internally, its dtype will be converted\n"," |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csc_matrix``.\n"," |      \n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The target values (class labels in classification, real numbers in\n"," |          regression).\n"," |      \n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights. If None, then samples are equally weighted. Splits\n"," |          that would create child nodes with net zero or negative weight are\n"," |          ignored while searching for a split in each node. In the case of\n"," |          classification, splits are also ignored if they would result in any\n"," |          single class carrying a negative weight in either child node.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from BaseForest:\n"," |  \n"," |  feature_importances_\n"," |      The impurity-based feature importances.\n"," |      \n"," |      The higher, the more important the feature.\n"," |      The importance of a feature is computed as the (normalized)\n"," |      total reduction of the criterion brought by that feature.  It is also\n"," |      known as the Gini importance.\n"," |      \n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      feature_importances_ : ndarray of shape (n_features,)\n"," |          The values of this array sum to 1, unless all trees are single node\n"," |          trees consisting of only the root node, in which case it will be an\n"," |          array of zeros.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n"," |  \n"," |  __getitem__(self, index)\n"," |      Return the index'th estimator in the ensemble.\n"," |  \n"," |  __iter__(self)\n"," |      Return iterator over estimators in the ensemble.\n"," |  \n"," |  __len__(self)\n"," |      Return the number of estimators in the ensemble.\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n"," |  \n"," |  __annotations__ = {'_required_parameters': typing.List[str]}\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.BaseEstimator:\n"," |  \n"," |  __getstate__(self)\n"," |  \n"," |  __repr__(self, N_CHAR_MAX=700)\n"," |      Return repr(self).\n"," |  \n"," |  __setstate__(self, state)\n"," |  \n"," |  get_params(self, deep=True)\n"," |      Get parameters for this estimator.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      deep : bool, default=True\n"," |          If True, will return the parameters for this estimator and\n"," |          contained subobjects that are estimators.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      params : dict\n"," |          Parameter names mapped to their values.\n"," |  \n"," |  set_params(self, **params)\n"," |      Set the parameters of this estimator.\n"," |      \n"," |      The method works on simple estimators as well as on nested objects\n"," |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n"," |      parameters of the form ``<component>__<parameter>`` so that it's\n"," |      possible to update each component of a nested object.\n"," |      \n"," |      Parameters\n"," |      ----------\n"," |      **params : dict\n"," |          Estimator parameters.\n"," |      \n"," |      Returns\n"," |      -------\n"," |      self : estimator instance\n"," |          Estimator instance.\n","\n"]}],"source":["help(RandomForestRegressor)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["rgs = RandomForestRegressor(n_estimators=5)\n","rgs = rgs.fit(boston_features,boston_target)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":["RandomForestRegressor(n_estimators=5)"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["rgs"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["26.580000000000002 --> 24.0\n","22.059999999999995 --> 21.6\n","34.5 --> 34.7\n","34.18 --> 33.4\n","36.620000000000005 --> 36.2\n","27.22 --> 28.7\n","22.1 --> 22.9\n","23.000000000000004 --> 27.1\n","16.56 --> 16.5\n","18.880000000000003 --> 18.9\n","18.94 --> 15.0\n","18.9 --> 18.9\n","21.7 --> 21.7\n","20.060000000000002 --> 20.4\n","18.2 --> 18.2\n","19.9 --> 19.9\n","22.5 --> 23.1\n","17.72 --> 17.5\n","21.5 --> 20.2\n","18.6 --> 18.2\n","13.24 --> 13.6\n","19.6 --> 19.6\n","16.68 --> 15.2\n","14.14 --> 14.5\n","15.6 --> 15.6\n","14.780000000000001 --> 13.9\n","16.48 --> 16.6\n","14.800000000000002 --> 14.8\n","19.6 --> 18.4\n","21.64 --> 21.0\n","14.079999999999998 --> 12.7\n","17.5 --> 14.5\n","13.2 --> 13.2\n","13.84 --> 13.1\n","13.440000000000001 --> 13.5\n","19.859999999999996 --> 18.9\n","19.48 --> 20.0\n","21.080000000000002 --> 21.0\n","22.78 --> 24.7\n","30.660000000000004 --> 30.8\n","33.24 --> 34.9\n","29.440000000000005 --> 26.6\n","24.34 --> 25.3\n","24.22 --> 24.7\n","20.660000000000004 --> 21.2\n","19.58 --> 19.3\n","19.9 --> 20.0\n","19.4 --> 16.6\n","15.559999999999999 --> 14.4\n","19.939999999999998 --> 19.4\n","19.7 --> 19.7\n","20.9 --> 20.5\n","24.880000000000003 --> 25.0\n","22.660000000000004 --> 23.4\n","19.18 --> 18.9\n","35.4 --> 35.4\n","23.26 --> 24.7\n","30.880000000000003 --> 31.6\n","23.02 --> 23.3\n","20.5 --> 19.6\n","19.1 --> 18.7\n","17.34 --> 16.0\n","22.96 --> 22.2\n","25.32 --> 25.0\n","33.0 --> 33.0\n","23.419999999999998 --> 23.5\n","19.4 --> 19.4\n","21.78 --> 22.0\n","17.839999999999996 --> 17.4\n","21.04 --> 20.9\n","24.48 --> 24.2\n","21.14 --> 21.7\n","22.32 --> 22.8\n","23.3 --> 23.4\n","24.1 --> 24.1\n","23.339999999999996 --> 21.4\n","19.759999999999998 --> 20.0\n","21.72 --> 20.8\n","20.96 --> 21.2\n","20.419999999999998 --> 20.3\n","27.8 --> 28.0\n","23.9 --> 23.9\n","22.880000000000003 --> 24.8\n","22.459999999999997 --> 22.9\n","22.9 --> 23.9\n","26.759999999999998 --> 26.6\n","22.06 --> 22.5\n","23.080000000000002 --> 22.2\n","25.640000000000004 --> 23.6\n","27.96 --> 28.7\n","22.5 --> 22.6\n","21.840000000000003 --> 22.0\n","22.459999999999997 --> 22.9\n","25.0 --> 25.0\n","20.68 --> 20.6\n","26.98 --> 28.4\n","20.82 --> 21.4\n","41.67999999999999 --> 38.7\n","43.4 --> 43.8\n","32.72 --> 33.2\n","27.5 --> 27.5\n","25.6 --> 26.5\n","18.080000000000002 --> 18.6\n","19.5 --> 19.3\n","19.7 --> 20.1\n","19.94 --> 19.5\n","20.080000000000002 --> 19.5\n","19.84 --> 20.4\n","19.74 --> 19.8\n","20.78 --> 19.4\n","21.22 --> 21.7\n","23.139999999999997 --> 22.8\n","18.74 --> 18.8\n","18.7 --> 18.7\n","19.78 --> 18.5\n","18.3 --> 18.3\n","20.62 --> 21.2\n","19.9 --> 19.2\n","20.939999999999998 --> 20.4\n","19.44 --> 19.3\n","20.32 --> 22.0\n","19.6 --> 20.3\n","20.22 --> 20.5\n","16.66 --> 17.3\n","19.14 --> 18.8\n","20.7 --> 21.4\n","16.380000000000003 --> 15.7\n","15.6 --> 16.2\n","17.119999999999997 --> 18.0\n","14.3 --> 14.3\n","20.72 --> 19.2\n","19.52 --> 19.6\n","23.0 --> 23.0\n","18.4 --> 18.4\n","15.34 --> 15.6\n","16.660000000000004 --> 18.1\n","16.339999999999996 --> 17.4\n","17.839999999999996 --> 17.1\n","13.720000000000002 --> 13.3\n","16.979999999999997 --> 17.8\n","15.16 --> 14.0\n","13.919999999999998 --> 14.4\n","13.8 --> 13.4\n","14.719999999999999 --> 15.6\n","11.8 --> 11.8\n","14.74 --> 13.8\n","15.6 --> 15.6\n","13.48 --> 14.6\n","15.320000000000002 --> 17.8\n","14.559999999999999 --> 15.4\n","21.5 --> 21.5\n","19.119999999999997 --> 19.6\n","16.54 --> 15.3\n","18.419999999999998 --> 19.4\n","17.880000000000003 --> 17.0\n","17.240000000000002 --> 15.6\n","14.419999999999998 --> 13.1\n","39.339999999999996 --> 41.3\n","24.16 --> 24.3\n","24.34 --> 23.3\n","26.46 --> 27.0\n","50.0 --> 50.0\n","50.0 --> 50.0\n","48.62 --> 50.0\n","22.7 --> 22.7\n","24.759999999999998 --> 25.0\n","50.0 --> 50.0\n","22.96 --> 23.8\n","23.639999999999997 --> 23.8\n","22.6 --> 22.3\n","20.4 --> 17.4\n","20.76 --> 19.1\n","23.1 --> 23.1\n","23.660000000000004 --> 23.6\n","22.72 --> 22.6\n","29.22 --> 29.4\n","22.660000000000004 --> 23.2\n","25.04 --> 24.6\n","28.02 --> 29.9\n","34.48 --> 37.2\n","41.64 --> 39.8\n","32.24 --> 36.2\n","37.9 --> 37.9\n","30.4 --> 32.5\n","26.639999999999997 --> 26.4\n","30.919999999999998 --> 29.6\n","49.339999999999996 --> 50.0\n","27.78 --> 32.0\n","33.019999999999996 --> 29.8\n","34.9 --> 34.9\n","36.22 --> 37.0\n","31.419999999999998 --> 30.5\n","34.779999999999994 --> 36.4\n","32.28 --> 31.1\n","29.98 --> 29.1\n","47.160000000000004 --> 50.0\n","33.61999999999999 --> 33.3\n","30.839999999999996 --> 30.3\n","34.6 --> 34.6\n","33.72 --> 34.9\n","33.4 --> 32.9\n","22.98 --> 24.1\n","42.3 --> 42.3\n","47.86 --> 48.5\n","48.46 --> 50.0\n","22.160000000000004 --> 22.6\n","23.8 --> 24.4\n","21.240000000000002 --> 22.5\n","24.4 --> 24.4\n","19.119999999999997 --> 20.0\n","20.7 --> 21.7\n","19.32 --> 19.3\n","22.419999999999998 --> 22.4\n","25.56 --> 28.1\n","21.46 --> 23.7\n","24.86 --> 25.0\n","22.979999999999997 --> 23.3\n","26.920000000000005 --> 28.7\n","19.419999999999998 --> 21.5\n","23.16 --> 23.0\n","26.82 --> 26.7\n","20.799999999999997 --> 21.7\n","26.68 --> 27.5\n","27.22 --> 30.1\n","45.18000000000001 --> 44.8\n","47.96 --> 50.0\n","41.56 --> 37.6\n","32.26 --> 31.6\n","45.92 --> 46.7\n","33.12 --> 31.5\n","24.3 --> 24.3\n","31.660000000000004 --> 31.7\n","44.6 --> 41.7\n","47.6 --> 48.3\n","27.839999999999996 --> 29.0\n","23.68 --> 24.0\n","25.879999999999995 --> 25.1\n","31.619999999999997 --> 31.5\n","23.84 --> 23.7\n","24.46 --> 23.3\n","22.0 --> 22.0\n","20.02 --> 20.1\n","22.419999999999998 --> 22.2\n","23.84 --> 23.7\n","17.28 --> 17.6\n","18.779999999999998 --> 18.5\n","22.779999999999998 --> 24.3\n","21.4 --> 20.5\n","24.419999999999998 --> 24.5\n","26.46 --> 26.2\n","24.4 --> 24.4\n","24.119999999999997 --> 24.8\n","27.96 --> 29.6\n","40.04 --> 42.8\n","22.26 --> 21.9\n","19.04 --> 20.9\n","43.28 --> 44.0\n","49.46 --> 50.0\n","30.619999999999997 --> 36.0\n","32.82 --> 30.1\n","33.46 --> 33.8\n","43.019999999999996 --> 43.1\n","48.14 --> 48.8\n","33.660000000000004 --> 31.0\n","34.9 --> 36.5\n","22.759999999999998 --> 22.8\n","26.139999999999997 --> 30.7\n","50.0 --> 50.0\n","45.18 --> 43.5\n","20.68 --> 20.7\n","21.1 --> 21.1\n","24.64 --> 25.2\n","24.4 --> 24.4\n","39.160000000000004 --> 35.2\n","30.98 --> 32.4\n","30.339999999999996 --> 32.0\n","33.0 --> 33.2\n","32.5 --> 33.1\n","27.339999999999996 --> 29.1\n","34.28 --> 35.1\n","46.440000000000005 --> 45.4\n","33.08 --> 35.4\n","46.8 --> 46.0\n","46.92 --> 50.0\n","31.18 --> 32.2\n","19.9 --> 22.0\n","19.580000000000002 --> 20.1\n","23.020000000000003 --> 23.2\n","22.48 --> 22.3\n","23.88 --> 24.8\n","29.74 --> 28.5\n","36.919999999999995 --> 37.3\n","29.2 --> 27.9\n","23.32 --> 23.9\n","22.04 --> 21.7\n","27.74 --> 28.6\n","27.100000000000005 --> 27.1\n","20.86 --> 20.3\n","22.7 --> 22.5\n","30.580000000000002 --> 29.0\n","24.1 --> 24.8\n","24.78 --> 22.0\n","26.4 --> 26.4\n","31.339999999999996 --> 33.1\n","35.019999999999996 --> 36.1\n","27.240000000000002 --> 28.4\n","33.92 --> 33.4\n","27.78 --> 28.2\n","28.419999999999998 --> 22.8\n","19.72 --> 20.3\n","17.9 --> 16.1\n","22.68 --> 22.1\n","19.4 --> 19.4\n","20.56 --> 21.6\n","23.76 --> 23.8\n","16.619999999999997 --> 16.2\n","17.8 --> 17.8\n","19.52 --> 19.8\n","22.160000000000004 --> 23.1\n","22.48 --> 21.0\n","24.52 --> 23.8\n","23.54 --> 23.1\n","20.939999999999998 --> 20.4\n","18.46 --> 18.5\n","24.6 --> 25.0\n","27.22 --> 24.6\n","22.48 --> 23.0\n","21.98 --> 22.2\n","20.259999999999998 --> 19.3\n","22.839999999999996 --> 22.6\n","21.779999999999998 --> 19.8\n","17.1 --> 17.1\n","18.939999999999998 --> 19.4\n","22.86 --> 22.2\n","22.080000000000002 --> 20.7\n","20.4 --> 21.1\n","19.52 --> 19.5\n","18.84 --> 18.5\n","20.6 --> 20.6\n","19.26 --> 19.0\n","18.86 --> 18.7\n","33.67999999999999 --> 32.7\n","20.12 --> 16.5\n","24.619999999999997 --> 23.9\n","31.439999999999998 --> 31.2\n","17.96 --> 17.5\n","17.259999999999998 --> 17.2\n","23.620000000000005 --> 23.1\n","25.139999999999997 --> 24.5\n","27.52 --> 26.6\n","23.499999999999993 --> 22.9\n","24.6 --> 24.1\n","20.12 --> 18.6\n","30.4 --> 30.1\n","18.96 --> 18.2\n","22.5 --> 20.6\n","17.0 --> 17.8\n","21.240000000000002 --> 21.7\n","22.18 --> 22.7\n","21.119999999999997 --> 22.6\n","22.54 --> 25.0\n","21.54 --> 19.9\n","23.259999999999998 --> 20.8\n","17.78 --> 16.8\n","37.86 --> 21.9\n","25.26 --> 27.5\n","21.7 --> 21.9\n","19.96 --> 23.1\n","44.14 --> 50.0\n","50.0 --> 50.0\n","45.8 --> 50.0\n","43.36 --> 50.0\n","43.36 --> 50.0\n","12.720000000000002 --> 13.8\n","12.419999999999998 --> 13.8\n","29.0 --> 15.0\n","13.5 --> 13.9\n","14.1 --> 13.3\n","15.979999999999999 --> 13.1\n","15.74 --> 10.2\n","10.4 --> 10.4\n","13.48 --> 10.9\n","11.14 --> 11.3\n","10.879999999999999 --> 12.3\n","9.120000000000001 --> 8.8\n","7.2 --> 7.2\n","9.18 --> 10.5\n","8.58 --> 7.4\n","9.02 --> 10.2\n","12.239999999999998 --> 11.5\n","15.1 --> 15.1\n","21.06 --> 23.2\n","9.440000000000001 --> 9.7\n","14.739999999999998 --> 13.8\n","11.08 --> 12.7\n","13.24 --> 13.1\n","12.5 --> 12.5\n","10.22 --> 8.5\n","6.5200000000000005 --> 5.0\n","7.44 --> 6.3\n","6.5600000000000005 --> 5.6\n","8.180000000000001 --> 7.2\n","11.38 --> 12.1\n","9.22 --> 8.3\n","7.859999999999999 --> 8.5\n","6.62 --> 5.0\n","13.4 --> 11.9\n","36.74 --> 27.9\n","16.42 --> 17.2\n","18.52 --> 27.5\n","19.2 --> 15.0\n","14.540000000000001 --> 17.2\n","14.76 --> 17.9\n","14.479999999999999 --> 16.3\n","7.640000000000001 --> 7.0\n","7.8 --> 7.2\n","7.9 --> 7.5\n","10.4 --> 10.4\n","8.8 --> 8.8\n","11.42 --> 8.4\n","15.780000000000001 --> 16.7\n","17.4 --> 14.2\n","20.839999999999996 --> 20.8\n","14.540000000000001 --> 13.4\n","14.839999999999998 --> 11.7\n","8.8 --> 8.3\n","12.44 --> 10.2\n","14.6 --> 10.9\n","13.2 --> 11.0\n","9.5 --> 9.5\n","15.16 --> 14.5\n","13.680000000000001 --> 14.1\n","16.840000000000003 --> 16.1\n","14.699999999999998 --> 14.3\n","13.280000000000001 --> 11.7\n","12.76 --> 13.4\n","10.48 --> 9.6\n","8.34 --> 8.7\n","8.84 --> 8.4\n","11.84 --> 12.8\n","10.5 --> 10.5\n","15.7 --> 17.1\n","17.439999999999998 --> 18.4\n","15.0 --> 15.4\n","10.74 --> 10.8\n","11.8 --> 11.8\n","14.52 --> 14.9\n","15.419999999999998 --> 12.6\n","13.8 --> 14.1\n","13.520000000000001 --> 13.0\n","13.4 --> 13.4\n","15.02 --> 15.2\n","15.560000000000002 --> 16.1\n","19.939999999999998 --> 17.8\n","14.88 --> 14.9\n","15.080000000000002 --> 14.1\n","14.680000000000001 --> 12.7\n","14.919999999999998 --> 13.5\n","16.240000000000002 --> 14.9\n","19.1 --> 20.0\n","16.24 --> 16.4\n","18.46 --> 17.7\n","20.2 --> 19.5\n","20.06 --> 20.2\n","22.139999999999997 --> 21.4\n","20.159999999999997 --> 19.9\n","18.16 --> 19.0\n","15.740000000000004 --> 19.1\n","16.939999999999998 --> 19.1\n","18.440000000000005 --> 20.1\n","19.04 --> 19.9\n","20.28 --> 19.6\n","19.3 --> 23.2\n","27.02 --> 29.8\n","14.139999999999997 --> 13.8\n","15.7 --> 13.3\n","15.8 --> 16.7\n","12.0 --> 12.0\n","14.6 --> 14.6\n","21.1 --> 21.4\n","22.94 --> 23.0\n","24.880000000000003 --> 23.7\n","30.8 --> 25.0\n","21.88 --> 21.8\n","19.82 --> 20.6\n","21.64 --> 21.2\n","19.32 --> 19.1\n","20.82 --> 20.6\n","14.6 --> 15.2\n","7.44 --> 7.0\n","7.7 --> 8.1\n","13.6 --> 13.6\n","19.939999999999998 --> 20.1\n","21.32 --> 21.8\n","22.880000000000003 --> 24.5\n","23.1 --> 23.1\n","18.48 --> 19.7\n","18.64 --> 18.3\n","21.1 --> 21.2\n","19.939999999999998 --> 17.5\n","17.92 --> 16.8\n","20.3 --> 22.4\n","12.819999999999999 --> 20.6\n","24.86 --> 23.9\n","22.0 --> 22.0\n","11.9 --> 11.9\n","均方误差 1.8505119471117073\n"]}],"source":["pred = rgs.predict(boston_features)\n","for i in range(pred.shape[0]):\n","    print(pred[i],'-->',boston_target[i])\n","print(\"均方误差\",np.sqrt(((pred-boston_target)**2).mean()))"]},{"cell_type":"markdown","metadata":{},"source":["### 使用决策树比较"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["from sklearn import tree"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"data":{"text/plain":["DecisionTreeRegressor()"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["dtr = tree.DecisionTreeRegressor()\n","dtr.fit(boston_features,boston_target)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["24.0 --> 24.0\n","21.6 --> 21.6\n","34.7 --> 34.7\n","33.4 --> 33.4\n","36.2 --> 36.2\n","28.7 --> 28.7\n","22.9 --> 22.9\n","27.1 --> 27.1\n","16.5 --> 16.5\n","18.9 --> 18.9\n","15.0 --> 15.0\n","18.9 --> 18.9\n","21.7 --> 21.7\n","20.4 --> 20.4\n","18.2 --> 18.2\n","19.9 --> 19.9\n","23.1 --> 23.1\n","17.5 --> 17.5\n","20.2 --> 20.2\n","18.2 --> 18.2\n","13.6 --> 13.6\n","19.6 --> 19.6\n","15.2 --> 15.2\n","14.5 --> 14.5\n","15.6 --> 15.6\n","13.9 --> 13.9\n","16.6 --> 16.6\n","14.8 --> 14.8\n","18.4 --> 18.4\n","21.0 --> 21.0\n","12.7 --> 12.7\n","14.5 --> 14.5\n","13.2 --> 13.2\n","13.1 --> 13.1\n","13.5 --> 13.5\n","18.9 --> 18.9\n","20.0 --> 20.0\n","21.0 --> 21.0\n","24.7 --> 24.7\n","30.8 --> 30.8\n","34.9 --> 34.9\n","26.6 --> 26.6\n","25.3 --> 25.3\n","24.7 --> 24.7\n","21.2 --> 21.2\n","19.3 --> 19.3\n","20.0 --> 20.0\n","16.6 --> 16.6\n","14.4 --> 14.4\n","19.4 --> 19.4\n","19.7 --> 19.7\n","20.5 --> 20.5\n","25.0 --> 25.0\n","23.4 --> 23.4\n","18.9 --> 18.9\n","35.4 --> 35.4\n","24.7 --> 24.7\n","31.6 --> 31.6\n","23.3 --> 23.3\n","19.6 --> 19.6\n","18.7 --> 18.7\n","16.0 --> 16.0\n","22.2 --> 22.2\n","25.0 --> 25.0\n","33.0 --> 33.0\n","23.5 --> 23.5\n","19.4 --> 19.4\n","22.0 --> 22.0\n","17.4 --> 17.4\n","20.9 --> 20.9\n","24.2 --> 24.2\n","21.7 --> 21.7\n","22.8 --> 22.8\n","23.4 --> 23.4\n","24.1 --> 24.1\n","21.4 --> 21.4\n","20.0 --> 20.0\n","20.8 --> 20.8\n","21.2 --> 21.2\n","20.3 --> 20.3\n","28.0 --> 28.0\n","23.9 --> 23.9\n","24.8 --> 24.8\n","22.9 --> 22.9\n","23.9 --> 23.9\n","26.6 --> 26.6\n","22.5 --> 22.5\n","22.2 --> 22.2\n","23.6 --> 23.6\n","28.7 --> 28.7\n","22.6 --> 22.6\n","22.0 --> 22.0\n","22.9 --> 22.9\n","25.0 --> 25.0\n","20.6 --> 20.6\n","28.4 --> 28.4\n","21.4 --> 21.4\n","38.7 --> 38.7\n","43.8 --> 43.8\n","33.2 --> 33.2\n","27.5 --> 27.5\n","26.5 --> 26.5\n","18.6 --> 18.6\n","19.3 --> 19.3\n","20.1 --> 20.1\n","19.5 --> 19.5\n","19.5 --> 19.5\n","20.4 --> 20.4\n","19.8 --> 19.8\n","19.4 --> 19.4\n","21.7 --> 21.7\n","22.8 --> 22.8\n","18.8 --> 18.8\n","18.7 --> 18.7\n","18.5 --> 18.5\n","18.3 --> 18.3\n","21.2 --> 21.2\n","19.2 --> 19.2\n","20.4 --> 20.4\n","19.3 --> 19.3\n","22.0 --> 22.0\n","20.3 --> 20.3\n","20.5 --> 20.5\n","17.3 --> 17.3\n","18.8 --> 18.8\n","21.4 --> 21.4\n","15.7 --> 15.7\n","16.2 --> 16.2\n","18.0 --> 18.0\n","14.3 --> 14.3\n","19.2 --> 19.2\n","19.6 --> 19.6\n","23.0 --> 23.0\n","18.4 --> 18.4\n","15.6 --> 15.6\n","18.1 --> 18.1\n","17.4 --> 17.4\n","17.1 --> 17.1\n","13.3 --> 13.3\n","17.8 --> 17.8\n","14.0 --> 14.0\n","14.4 --> 14.4\n","13.4 --> 13.4\n","15.6 --> 15.6\n","11.8 --> 11.8\n","13.8 --> 13.8\n","15.6 --> 15.6\n","14.6 --> 14.6\n","17.8 --> 17.8\n","15.4 --> 15.4\n","21.5 --> 21.5\n","19.6 --> 19.6\n","15.3 --> 15.3\n","19.4 --> 19.4\n","17.0 --> 17.0\n","15.6 --> 15.6\n","13.1 --> 13.1\n","41.3 --> 41.3\n","24.3 --> 24.3\n","23.3 --> 23.3\n","27.0 --> 27.0\n","50.0 --> 50.0\n","50.0 --> 50.0\n","50.0 --> 50.0\n","22.7 --> 22.7\n","25.0 --> 25.0\n","50.0 --> 50.0\n","23.8 --> 23.8\n","23.8 --> 23.8\n","22.3 --> 22.3\n","17.4 --> 17.4\n","19.1 --> 19.1\n","23.1 --> 23.1\n","23.6 --> 23.6\n","22.6 --> 22.6\n","29.4 --> 29.4\n","23.2 --> 23.2\n","24.6 --> 24.6\n","29.9 --> 29.9\n","37.2 --> 37.2\n","39.8 --> 39.8\n","36.2 --> 36.2\n","37.9 --> 37.9\n","32.5 --> 32.5\n","26.4 --> 26.4\n","29.6 --> 29.6\n","50.0 --> 50.0\n","32.0 --> 32.0\n","29.8 --> 29.8\n","34.9 --> 34.9\n","37.0 --> 37.0\n","30.5 --> 30.5\n","36.4 --> 36.4\n","31.1 --> 31.1\n","29.1 --> 29.1\n","50.0 --> 50.0\n","33.3 --> 33.3\n","30.3 --> 30.3\n","34.6 --> 34.6\n","34.9 --> 34.9\n","32.9 --> 32.9\n","24.1 --> 24.1\n","42.3 --> 42.3\n","48.5 --> 48.5\n","50.0 --> 50.0\n","22.6 --> 22.6\n","24.4 --> 24.4\n","22.5 --> 22.5\n","24.4 --> 24.4\n","20.0 --> 20.0\n","21.7 --> 21.7\n","19.3 --> 19.3\n","22.4 --> 22.4\n","28.1 --> 28.1\n","23.7 --> 23.7\n","25.0 --> 25.0\n","23.3 --> 23.3\n","28.7 --> 28.7\n","21.5 --> 21.5\n","23.0 --> 23.0\n","26.7 --> 26.7\n","21.7 --> 21.7\n","27.5 --> 27.5\n","30.1 --> 30.1\n","44.8 --> 44.8\n","50.0 --> 50.0\n","37.6 --> 37.6\n","31.6 --> 31.6\n","46.7 --> 46.7\n","31.5 --> 31.5\n","24.3 --> 24.3\n","31.7 --> 31.7\n","41.7 --> 41.7\n","48.3 --> 48.3\n","29.0 --> 29.0\n","24.0 --> 24.0\n","25.1 --> 25.1\n","31.5 --> 31.5\n","23.7 --> 23.7\n","23.3 --> 23.3\n","22.0 --> 22.0\n","20.1 --> 20.1\n","22.2 --> 22.2\n","23.7 --> 23.7\n","17.6 --> 17.6\n","18.5 --> 18.5\n","24.3 --> 24.3\n","20.5 --> 20.5\n","24.5 --> 24.5\n","26.2 --> 26.2\n","24.4 --> 24.4\n","24.8 --> 24.8\n","29.6 --> 29.6\n","42.8 --> 42.8\n","21.9 --> 21.9\n","20.9 --> 20.9\n","44.0 --> 44.0\n","50.0 --> 50.0\n","36.0 --> 36.0\n","30.1 --> 30.1\n","33.8 --> 33.8\n","43.1 --> 43.1\n","48.8 --> 48.8\n","31.0 --> 31.0\n","36.5 --> 36.5\n","22.8 --> 22.8\n","30.7 --> 30.7\n","50.0 --> 50.0\n","43.5 --> 43.5\n","20.7 --> 20.7\n","21.1 --> 21.1\n","25.2 --> 25.2\n","24.4 --> 24.4\n","35.2 --> 35.2\n","32.4 --> 32.4\n","32.0 --> 32.0\n","33.2 --> 33.2\n","33.1 --> 33.1\n","29.1 --> 29.1\n","35.1 --> 35.1\n","45.4 --> 45.4\n","35.4 --> 35.4\n","46.0 --> 46.0\n","50.0 --> 50.0\n","32.2 --> 32.2\n","22.0 --> 22.0\n","20.1 --> 20.1\n","23.2 --> 23.2\n","22.3 --> 22.3\n","24.8 --> 24.8\n","28.5 --> 28.5\n","37.3 --> 37.3\n","27.9 --> 27.9\n","23.9 --> 23.9\n","21.7 --> 21.7\n","28.6 --> 28.6\n","27.1 --> 27.1\n","20.3 --> 20.3\n","22.5 --> 22.5\n","29.0 --> 29.0\n","24.8 --> 24.8\n","22.0 --> 22.0\n","26.4 --> 26.4\n","33.1 --> 33.1\n","36.1 --> 36.1\n","28.4 --> 28.4\n","33.4 --> 33.4\n","28.2 --> 28.2\n","22.8 --> 22.8\n","20.3 --> 20.3\n","16.1 --> 16.1\n","22.1 --> 22.1\n","19.4 --> 19.4\n","21.6 --> 21.6\n","23.8 --> 23.8\n","16.2 --> 16.2\n","17.8 --> 17.8\n","19.8 --> 19.8\n","23.1 --> 23.1\n","21.0 --> 21.0\n","23.8 --> 23.8\n","23.1 --> 23.1\n","20.4 --> 20.4\n","18.5 --> 18.5\n","25.0 --> 25.0\n","24.6 --> 24.6\n","23.0 --> 23.0\n","22.2 --> 22.2\n","19.3 --> 19.3\n","22.6 --> 22.6\n","19.8 --> 19.8\n","17.1 --> 17.1\n","19.4 --> 19.4\n","22.2 --> 22.2\n","20.7 --> 20.7\n","21.1 --> 21.1\n","19.5 --> 19.5\n","18.5 --> 18.5\n","20.6 --> 20.6\n","19.0 --> 19.0\n","18.7 --> 18.7\n","32.7 --> 32.7\n","16.5 --> 16.5\n","23.9 --> 23.9\n","31.2 --> 31.2\n","17.5 --> 17.5\n","17.2 --> 17.2\n","23.1 --> 23.1\n","24.5 --> 24.5\n","26.6 --> 26.6\n","22.9 --> 22.9\n","24.1 --> 24.1\n","18.6 --> 18.6\n","30.1 --> 30.1\n","18.2 --> 18.2\n","20.6 --> 20.6\n","17.8 --> 17.8\n","21.7 --> 21.7\n","22.7 --> 22.7\n","22.6 --> 22.6\n","25.0 --> 25.0\n","19.9 --> 19.9\n","20.8 --> 20.8\n","16.8 --> 16.8\n","21.9 --> 21.9\n","27.5 --> 27.5\n","21.9 --> 21.9\n","23.1 --> 23.1\n","50.0 --> 50.0\n","50.0 --> 50.0\n","50.0 --> 50.0\n","50.0 --> 50.0\n","50.0 --> 50.0\n","13.8 --> 13.8\n","13.8 --> 13.8\n","15.0 --> 15.0\n","13.9 --> 13.9\n","13.3 --> 13.3\n","13.1 --> 13.1\n","10.2 --> 10.2\n","10.4 --> 10.4\n","10.9 --> 10.9\n","11.3 --> 11.3\n","12.3 --> 12.3\n","8.8 --> 8.8\n","7.2 --> 7.2\n","10.5 --> 10.5\n","7.4 --> 7.4\n","10.2 --> 10.2\n","11.5 --> 11.5\n","15.1 --> 15.1\n","23.2 --> 23.2\n","9.7 --> 9.7\n","13.8 --> 13.8\n","12.7 --> 12.7\n","13.1 --> 13.1\n","12.5 --> 12.5\n","8.5 --> 8.5\n","5.0 --> 5.0\n","6.3 --> 6.3\n","5.6 --> 5.6\n","7.2 --> 7.2\n","12.1 --> 12.1\n","8.3 --> 8.3\n","8.5 --> 8.5\n","5.0 --> 5.0\n","11.9 --> 11.9\n","27.9 --> 27.9\n","17.2 --> 17.2\n","27.5 --> 27.5\n","15.0 --> 15.0\n","17.2 --> 17.2\n","17.9 --> 17.9\n","16.3 --> 16.3\n","7.0 --> 7.0\n","7.2 --> 7.2\n","7.5 --> 7.5\n","10.4 --> 10.4\n","8.8 --> 8.8\n","8.4 --> 8.4\n","16.7 --> 16.7\n","14.2 --> 14.2\n","20.8 --> 20.8\n","13.4 --> 13.4\n","11.7 --> 11.7\n","8.3 --> 8.3\n","10.2 --> 10.2\n","10.9 --> 10.9\n","11.0 --> 11.0\n","9.5 --> 9.5\n","14.5 --> 14.5\n","14.1 --> 14.1\n","16.1 --> 16.1\n","14.3 --> 14.3\n","11.7 --> 11.7\n","13.4 --> 13.4\n","9.6 --> 9.6\n","8.7 --> 8.7\n","8.4 --> 8.4\n","12.8 --> 12.8\n","10.5 --> 10.5\n","17.1 --> 17.1\n","18.4 --> 18.4\n","15.4 --> 15.4\n","10.8 --> 10.8\n","11.8 --> 11.8\n","14.9 --> 14.9\n","12.6 --> 12.6\n","14.1 --> 14.1\n","13.0 --> 13.0\n","13.4 --> 13.4\n","15.2 --> 15.2\n","16.1 --> 16.1\n","17.8 --> 17.8\n","14.9 --> 14.9\n","14.1 --> 14.1\n","12.7 --> 12.7\n","13.5 --> 13.5\n","14.9 --> 14.9\n","20.0 --> 20.0\n","16.4 --> 16.4\n","17.7 --> 17.7\n","19.5 --> 19.5\n","20.2 --> 20.2\n","21.4 --> 21.4\n","19.9 --> 19.9\n","19.0 --> 19.0\n","19.1 --> 19.1\n","19.1 --> 19.1\n","20.1 --> 20.1\n","19.9 --> 19.9\n","19.6 --> 19.6\n","23.2 --> 23.2\n","29.8 --> 29.8\n","13.8 --> 13.8\n","13.3 --> 13.3\n","16.7 --> 16.7\n","12.0 --> 12.0\n","14.6 --> 14.6\n","21.4 --> 21.4\n","23.0 --> 23.0\n","23.7 --> 23.7\n","25.0 --> 25.0\n","21.8 --> 21.8\n","20.6 --> 20.6\n","21.2 --> 21.2\n","19.1 --> 19.1\n","20.6 --> 20.6\n","15.2 --> 15.2\n","7.0 --> 7.0\n","8.1 --> 8.1\n","13.6 --> 13.6\n","20.1 --> 20.1\n","21.8 --> 21.8\n","24.5 --> 24.5\n","23.1 --> 23.1\n","19.7 --> 19.7\n","18.3 --> 18.3\n","21.2 --> 21.2\n","17.5 --> 17.5\n","16.8 --> 16.8\n","22.4 --> 22.4\n","20.6 --> 20.6\n","23.9 --> 23.9\n","22.0 --> 22.0\n","11.9 --> 11.9\n","均方误差 0.0\n"]}],"source":["pred = dtr.predict(boston_features)\n","for i in range(pred.shape[0]):\n","    print(pred[i],'-->',boston_target[i])\n","print(\"均方误差\",np.sqrt(((pred-boston_target)**2).mean()))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"d46e21d0935fd83c4ac28d914742846a2fe41c9b46e0d19e8805f73b2486dc37"},"kernelspec":{"display_name":"Python 3.6.13 ('tianshou')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
